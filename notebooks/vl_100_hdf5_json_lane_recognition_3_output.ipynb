{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdhFmWDvXgH6",
        "outputId": "49e35b4a-53bd-461b-a0e9-b8f08dd1e489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7.12 (default, Jan 15 2022, 18:48:18) \n",
            "[GCC 7.5.0]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r1pMHdEC9EjJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import glob\n",
        "import json\n",
        "\n",
        "import cv2\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from typing import Tuple, List, Dict, Iterable\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "import logging\n",
        "import math\n",
        "from typing import Optional\n",
        "import random\n",
        "from pathlib import Path\n",
        "import h5py\n",
        "\n",
        "from typing import NamedTuple, Tuple, List\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install unrar"
      ],
      "metadata": {
        "id": "m45ggIE2EYKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ee03cf6-ec23-42dc-8741-3632117a40fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Collecting unrar\n",
            "  Downloading unrar-0.4-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: unrar\n",
            "Successfully installed unrar-0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "pdgYMK0cEYuy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "yk7a4Vr5CNWP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "htieeTB39JF_",
        "outputId": "766b972b-b6c0-40fe-d974-ddb05cce82f2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3b8a479202a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0muse_metadata_server\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_metadata_server\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       ephemeral=ephemeral)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server, ephemeral)\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 136\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMNUdLK39Kbl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unrar x -Y \"/content/drive/My Drive/Ilmenau/dataset.rar\" \"/tmp/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SHAPE = (1280, 960, 3)\n",
        "BATCH_SIZE = 32\n",
        "AMOUNT_OF_FRAMES = 10000\n",
        "VALIDATION_SPLIT = 0.2\n",
        "MAX_LINES_PER_FRAME = 2\n",
        "MAX_NUM_POINTS =  91\n",
        "NUM_TYPE_OF_LINES = 4"
      ],
      "metadata": {
        "id": "sHlIU6PhB2WR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = \"/tmp/dataset/VIL100/\"\n",
        "IMAGE_PATH = BASE_DIR + \"JPEGImages/\"\n",
        "JSON_PATH = BASE_DIR + \"Json/\"\n",
        "JSON_HDF5_DATASET_PATH = BASE_DIR + \"hdf5/\"\n",
        "\n",
        "print(IMAGE_PATH)\n",
        "print(JSON_PATH)\n",
        "print(JSON_HDF5_DATASET_PATH)"
      ],
      "metadata": {
        "id": "rJCbS-7dDAav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = glob.glob(IMAGE_PATH+'/*/*.jpg')\n",
        "json_files = glob.glob(JSON_PATH+'/*/*.json')\n",
        "json_glob_path = JSON_PATH + '/*/*.json'"
      ],
      "metadata": {
        "id": "OmOixe6VC9ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VIL100HDF5:\n",
        "    ROOT_FOLDER = 'hdf5'\n",
        "    GROUP_NAME = 'frame_polylines_labels'\n",
        "    POLYLINES_DATASET_NAME = 'polylines'\n",
        "    LABELS_DATASET_NAME = 'labels'\n",
        "LANE_ID_FULL_LIST = set(range(1, 3))\n",
        "\n",
        "\n",
        "class Vil100Json:\n",
        "    ANNOTATIONS = 'annotations'\n",
        "    ATTRIBUTE = 'attribute'\n",
        "    LANE = 'lane'\n",
        "    LANE_ID = 'lane_id'\n",
        "    POINTS = 'points'\n",
        "\n",
        "\n",
        "class VIL100Attribute:\n",
        "    \"\"\"Lane Attribute id (type lane) in jsons\"\"\"\n",
        "    SINGLE_WHITE_SOLID = 1\n",
        "    SINGLE_WHITE_DOTTED = 2\n",
        "    SINGLE_YELLOW_SOLID = 3\n",
        "    SINGLE_YELLOW_DOTTED = 4\n",
        "    DOUBLE_WHITE_SOLID = 5\n",
        "    DOUBLE_YELLOW_SOLID = 7\n",
        "    DOUBLE_YELLOW_DOTTED = 8\n",
        "    DOUBLE_WHITE_SOLID_DOTTED = 9\n",
        "    DOUBLE_WHITE_DOTTED_SOLID = 10\n",
        "    DOUBLE_SOLID_WHITE_AND_YELLOW = 13\n",
        "\n",
        "\n",
        "class LineType:\n",
        "    \"\"\"Type lane in our task\"\"\"\n",
        "    NO_LINE = 0\n",
        "    SINGLE_WHITE_SOLID = 1\n",
        "    SINGLE_WHITE_DOTTED = 2\n",
        "    DOUBLE_WHITE_SOLID = 3\n",
        "    ALL_LINES = {NO_LINE, SINGLE_WHITE_SOLID, SINGLE_WHITE_DOTTED, DOUBLE_WHITE_SOLID}\n",
        "\n",
        "\n",
        "VIL_100_colour_line = {\n",
        "    LineType.SINGLE_WHITE_SOLID: (255, 0, 0),  # single white solid\n",
        "    LineType.SINGLE_WHITE_DOTTED: (0, 255, 0),  # single white dotted\n",
        "    LineType.DOUBLE_WHITE_SOLID: (255, 125, 0),  # single yellow solid\n",
        "    # 4: (255, 255, 0),  # single yellow dotted\n",
        "    # 5: (255, 0, 0),  # double white solid\n",
        "    # 6: (255, 125, 0),  # double yellow solid\n",
        "    # 7: (255, 255, 0),  # double yellow dotted\n",
        "    # 8: (255, 0, 0),  # double white solid dotted\n",
        "    # 9: (255, 0, 0),  # double white dotted solid\n",
        "    # 10: (255, 0, 0),  # double solid white and yellow\n",
        "}\n",
        "\n",
        "\n",
        "def get_valid_attribute(attr: int) -> int:\n",
        "    \"\"\"Change attribute from VIL100 dataset to normal number without missings\"\"\"\n",
        "    _VIL_100_attributes = {\n",
        "        LineType.NO_LINE: LineType.NO_LINE,\n",
        "        VIL100Attribute.SINGLE_WHITE_SOLID: LineType.SINGLE_WHITE_SOLID,\n",
        "        VIL100Attribute.SINGLE_WHITE_DOTTED: LineType.SINGLE_WHITE_DOTTED,\n",
        "        VIL100Attribute.SINGLE_YELLOW_SOLID: LineType.SINGLE_WHITE_SOLID,\n",
        "        VIL100Attribute.SINGLE_YELLOW_DOTTED: LineType.SINGLE_WHITE_DOTTED,\n",
        "        VIL100Attribute.DOUBLE_WHITE_SOLID: LineType.DOUBLE_WHITE_SOLID,\n",
        "        VIL100Attribute.DOUBLE_YELLOW_SOLID: LineType.DOUBLE_WHITE_SOLID,\n",
        "        VIL100Attribute.DOUBLE_YELLOW_DOTTED: LineType.DOUBLE_WHITE_SOLID,\n",
        "        VIL100Attribute.DOUBLE_WHITE_SOLID_DOTTED: LineType.DOUBLE_WHITE_SOLID,\n",
        "        VIL100Attribute.DOUBLE_WHITE_DOTTED_SOLID: LineType.DOUBLE_WHITE_SOLID,\n",
        "        VIL100Attribute.DOUBLE_SOLID_WHITE_AND_YELLOW: LineType.DOUBLE_WHITE_SOLID,\n",
        "    }\n",
        "    return _VIL_100_attributes.get(attr, LineType.NO_LINE)\n",
        "\n",
        "\n",
        "def get_colour_from_one_hot_vector(vector: np.ndarray) -> Tuple[int, int, int]:\n",
        "    \"\"\"Get colour from one hot vector\"\"\"\n",
        "    return VIL_100_colour_line.get(int(np.argmax(vector, axis=1)), None)\n",
        "\n",
        "\n",
        "def one_hot_list_encoder(target_class_idx: int, num_classes: int) -> np.ndarray:\n",
        "    \"\"\"One-hot list encoder\"\"\"\n",
        "    target_vector = np.zeros(num_classes)\n",
        "    target_vector[target_class_idx] = 1\n",
        "    return target_vector"
      ],
      "metadata": {
        "id": "fyfgUKHeBmVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VILLJsonConverter:\n",
        "\n",
        "    def __init__(self,\n",
        "                 max_lines_per_frame: int,\n",
        "                 max_num_points: int,\n",
        "                 num_type_of_lines: int,\n",
        "                 json_glob_path: str = None, ):\n",
        "\n",
        "        self.max_lines_per_frame = max_lines_per_frame\n",
        "        self.max_num_points = max_num_points\n",
        "        self.num_type_of_lines = num_type_of_lines\n",
        "        self.json_files = sorted(glob.glob(json_glob_path))\n",
        "        self.files_count = len(self.json_files)\n",
        "\n",
        "    def __get_polyline_with_label(self, lane: dict) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Get array from points list\"\"\"\n",
        "        points = np.array(\n",
        "            lane[Vil100Json.POINTS]).flatten()\n",
        "        points = np.pad(points, pad_width=(0, self.max_num_points * 2 - points.shape[0]))\n",
        "        # TODO @Karim: remember below `label.get(label)` is index 1,2,3,4\n",
        "        label = get_valid_attribute(lane.get(Vil100Json.ATTRIBUTE, LineType.NO_LINE))\n",
        "        labels = one_hot_list_encoder(label, self.num_type_of_lines)\n",
        "        return points, labels\n",
        "\n",
        "    def __get_polyline_and_label_from_file(self, json_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Retrieve from json file polylines and labels and format to nn input\n",
        "\n",
        "        :param json_path: json file path\n",
        "        :return: frame and tuple of labels\n",
        "        \"\"\"\n",
        "        with open(json_path) as f:\n",
        "            lanes: List[Dict[str, int]] = json.load(f)[Vil100Json.ANNOTATIONS][Vil100Json.LANE]\n",
        "            lanes = sorted(lanes, key=lambda lane: lane[Vil100Json.LANE_ID])\n",
        "\n",
        "            if lanes:\n",
        "                polylines, labels = np.array([]), np.array([])\n",
        "                # TODO @Karim: check another params in json files like \"occlusion\"\n",
        "                exist_lane = [x[Vil100Json.LANE_ID] for x in lanes]\n",
        "                missed_lane = LANE_ID_FULL_LIST - set(exist_lane)\n",
        "\n",
        "                for lane_id in range(1, self.max_lines_per_frame + 1):\n",
        "                    if lane_id in missed_lane:\n",
        "                        points = np.zeros(shape=(self.max_num_points * 2))\n",
        "                        label = one_hot_list_encoder(LineType.NO_LINE, self.num_type_of_lines)\n",
        "                    else:\n",
        "                        points, label = self.__get_polyline_with_label(lane=lanes[exist_lane.index(lane_id)])\n",
        "\n",
        "                    if lane_id % 2 == 0:\n",
        "                        polylines = np.append(polylines, points)\n",
        "                        labels = np.append(labels, label)\n",
        "                    else:\n",
        "                        polylines = np.insert(polylines, 0, points)\n",
        "                        labels = np.insert(labels, 0, label)\n",
        "\n",
        "                return polylines, labels\n",
        "            else:\n",
        "                empty_label = one_hot_list_encoder(LineType.NO_LINE, self.num_type_of_lines)\n",
        "                polylines_empty_shape = self.max_lines_per_frame * self.max_num_points * 2\n",
        "                return np.zeros(shape=polylines_empty_shape), np.array(\n",
        "                    [empty_label for x in range(self.max_lines_per_frame)]).flatten()\n",
        "\n",
        "    def exec(self) -> None:\n",
        "        \"\"\"Convert and save json files to new hdf5 files\"\"\"\n",
        "        for json_file_path in self.json_files:\n",
        "            polylines, labels = self.__get_polyline_and_label_from_file(json_file_path)\n",
        "\n",
        "            full_path_list = json_file_path.split('/')\n",
        "            full_path_list[-3] = VIL100HDF5.ROOT_FOLDER\n",
        "            root_path = full_path_list[:-1]\n",
        "            frame_name = full_path_list[-1]\n",
        "\n",
        "            Path(f\"{'/'.join(root_path)}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            with h5py.File(f\"{'/'.join(root_path)}/{frame_name}.hdf5\", \"w\") as f:\n",
        "                grp = f.create_group(VIL100HDF5.GROUP_NAME)\n",
        "                grp.create_dataset(VIL100HDF5.POLYLINES_DATASET_NAME, data=polylines, dtype='int32')\n",
        "                grp.create_dataset(VIL100HDF5.LABELS_DATASET_NAME, data=labels, dtype='int32')\n"
      ],
      "metadata": {
        "id": "fbWFrqSmBpu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converter = VILLJsonConverter(\n",
        "        max_lines_per_frame=MAX_LINES_PER_FRAME,\n",
        "        max_num_points=MAX_NUM_POINTS,\n",
        "        num_type_of_lines=NUM_TYPE_OF_LINES,\n",
        "        json_glob_path=json_glob_path,\n",
        "    )"
      ],
      "metadata": {
        "id": "r40mnshXByAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converter.exec()"
      ],
      "metadata": {
        "id": "Y9clI0smB6C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejnvAiA5T1Yv"
      },
      "outputs": [],
      "source": [
        "def calculate_perspective_transform_matrix(width: int, height: int, reverse_flag=False) -> Tuple[\n",
        "    np.ndarray]:\n",
        "    \"\"\"\n",
        "    Calculate transformation matrix for perspective transformation\n",
        "    :param width: frame width\n",
        "    :param height: frame height\n",
        "    :param reverse_flag: create reverse matrix for reverting to initial frame\n",
        "    :return: matrix for transformation the frame\n",
        "    \"\"\"\n",
        "    # TODO @Karim: check on real Audi Q2 input frame\n",
        "    high_left_crd, high_right_crd = (550, 530), (700, 530)\n",
        "    down_left_crd, down_right_crd, = (0, height - 150), (width, height - 150)\n",
        "\n",
        "    initial_matrix = np.float32([[high_left_crd, high_right_crd,\n",
        "                                  down_left_crd, down_right_crd]])\n",
        "    final_matrix = np.float32([[(0, 0), (width, 0), (0, height), (width, height)]])\n",
        "\n",
        "    return cv2.getPerspectiveTransform(initial_matrix, final_matrix) \\\n",
        "        if not reverse_flag else cv2.getPerspectiveTransform(final_matrix, initial_matrix)\n",
        "\n",
        "\n",
        "def transform_frame(frame: np.ndarray, width: int, height: int, reverse_flag=False) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Perform perspective transformation\n",
        "    :param frame: frame\n",
        "    :param width: frame width\n",
        "    :param height: frame height\n",
        "    :param reverse_flag: cancel perspective transformation\n",
        "    :return: changed (un)transformed frame\n",
        "    \"\"\"\n",
        "    if not reverse_flag:\n",
        "        initial_matrix = calculate_perspective_transform_matrix(width, height)\n",
        "        frame = cv2.warpPerspective(frame, initial_matrix, dsize=(width, height))\n",
        "    else:\n",
        "        final_matrix = calculate_perspective_transform_matrix(width, height, reverse_flag=True)\n",
        "        frame = cv2.warpPerspective(frame, final_matrix, dsize=(width, height))\n",
        "    return frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjYjpIH8Yv6A"
      },
      "outputs": [],
      "source": [
        "class SimpleFrameGenerator(Sequence):\n",
        "    \"\"\"Sequence of frames generator\n",
        "\n",
        "    Usage for training NN that could process independent\n",
        "    frames without context window etc\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_type_of_lines=4,\n",
        "                 max_num_points=91,\n",
        "                 max_lines_per_frame=2,\n",
        "                 rescale=1 / 255.,  # TODO @Karim: include and use later\n",
        "                 batch_size: int = 8,\n",
        "                 target_shape: Tuple[int, int] = (1280, 960),\n",
        "                 shuffle: bool = False,\n",
        "                 nb_channel: int = 3,  # TODO: Use rgb later\n",
        "                 files: Optional[List[str]] = None,\n",
        "                 json_files: Optional[List[str]] = None):\n",
        "        \"\"\"\n",
        "        :param subset: training or validation data\n",
        "        :param max_lines_per_frame: maxinum number of lines per frame\n",
        "        :param max_num_points: maximum number of points un one polyline\n",
        "        :param num_type_of_lines: number of possible lines on road\n",
        "        :param rescale:\n",
        "        :param batch_size: batch size of the dataset\n",
        "        :param target_shape: final size for NN input\n",
        "        :param shuffle: shuffle flag of frames sequences\n",
        "        :param split: split dataset to train/test\n",
        "        :param nb_channel: grayscaled or RGB frames\n",
        "        :param frame_glob_path: glob pattern of frames\n",
        "        :param json_glob_path: glob pattern path of jsons\n",
        "        \"\"\"\n",
        "        self.max_lines_per_frame = max_lines_per_frame\n",
        "        self.max_num_points = max_num_points\n",
        "        self.num_type_of_lines = num_type_of_lines\n",
        "        self.rescale = rescale\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.target_shape = target_shape\n",
        "        self.nb_channel = nb_channel\n",
        "        self.files = files\n",
        "        self.json_files = json_files\n",
        "        self.files_count = len(self.files)\n",
        "\n",
        "        if shuffle:\n",
        "            temp = list(zip(self.files, self.json_files))\n",
        "            random.shuffle(temp)\n",
        "            self.files, self.json_files = zip(*temp)\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(self.files_count / self.batch_size)\n",
        "\n",
        "    def __get_polyline_and_label_from_file(self, json_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Get from hdf5 all polylines and their labels\n",
        "        :param json_path: path of json file\n",
        "        :return: polylines with labels\n",
        "        \"\"\"\n",
        "        file = h5py.File(json_path, 'r')\n",
        "        group = file.get(VIL100HDF5.GROUP_NAME)\n",
        "        return group.get(VIL100HDF5.POLYLINES_DATASET_NAME), group.get(VIL100HDF5.LABELS_DATASET_NAME)\n",
        "\n",
        "    def __getitem__(self, idx) -> Tuple[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n",
        "        batch_frames_path = self.files[idx * self.batch_size:\n",
        "                                       (idx + 1) * self.batch_size]\n",
        "        batch_json_path = self.json_files[idx * self.batch_size:\n",
        "                                          (idx + 1) * self.batch_size]\n",
        "\n",
        "        polylines_list, labels_list = self.__get_polyline_and_label_from_file(batch_json_path[0])\n",
        "        for _json in batch_json_path[1:]:\n",
        "            polylines, labels = self.__get_polyline_and_label_from_file(_json)\n",
        "            polylines_list = np.vstack((polylines_list, polylines))\n",
        "            labels_list = np.vstack((labels_list, labels))\n",
        "\n",
        "        return np.array([\n",
        "            resize(imread(file_name) * self.rescale, self.target_shape) for file_name in\n",
        "            batch_frames_path]), (polylines_list,) + tuple(np.hsplit(labels_list, self.max_lines_per_frame))\n",
        "\n",
        "\n",
        "class SimpleFrameDataGen:\n",
        "    TRAINING = 'training'\n",
        "    VALIDATION = 'validation'\n",
        "\n",
        "    __reverse_dataset_type = {\n",
        "        TRAINING: VALIDATION,\n",
        "        VALIDATION: TRAINING\n",
        "    }\n",
        "    __dataset = {}\n",
        "\n",
        "    def __init__(self,\n",
        "                 rescale=1 / 255.,\n",
        "                 validation_split: Optional[float] = None,\n",
        "                 frame_glob_path: str = \"\",\n",
        "                 json_hdf5_glob_path: str = \"\"):\n",
        "        \"\"\"\n",
        "        :param validation_split: split for train/validation sets\n",
        "        :param rescale:\n",
        "        :param frame_glob_path: glob pattern of frames\n",
        "        :param json_glob_path: glob pattern path of jsons\n",
        "        \"\"\"\n",
        "        self.rescale = rescale\n",
        "        self.validation_split = validation_split\n",
        "\n",
        "        self.__frame_glob_path = frame_glob_path\n",
        "        self.__json_hdf5_glob_path = json_hdf5_glob_path\n",
        "\n",
        "    def flow_from_directory(self, subset: str = TRAINING,\n",
        "                            shuffle: bool = True, number_files: int = 2000, *args, **kwargs) -> SimpleFrameGenerator:\n",
        "        \"\"\"\n",
        "        Get generator for subset\n",
        "        :param subset: 'training' or 'validation'\n",
        "        :param shuffle: flag for shuffling\n",
        "        :param number_files: rectrict max number of files from dataset\n",
        "        :param args: args for specific dataset\n",
        "        :param kwargs: kwargs for specific dataset\n",
        "        :return: Specific generator for specific subset\n",
        "        \"\"\"\n",
        "\n",
        "        files = sorted(glob.glob(self.__frame_glob_path))\n",
        "        log.info(f\"Number of files in dataset: {len(files)}. Using in training/validation: {number_files}\")\n",
        "        files = files[:number_files]\n",
        "\n",
        "        json_files = sorted(glob.glob(self.__json_hdf5_glob_path))[:number_files]\n",
        "        files_count = len(files)\n",
        "        json_files_count = len(json_files)\n",
        "\n",
        "        if files_count != json_files_count:\n",
        "            log.error(f\"Dataset files error\"\n",
        "                      f\"Number of frames: ({files_count}). \"\n",
        "                      f\"Number of jsons({json_files_count}\")\n",
        "            raise FileNotFoundError(\n",
        "                f\"Numbers of frames and jsons are not equal!\")\n",
        "\n",
        "        if not self.__reverse_dataset_type.get(subset):\n",
        "            log.error(f'Wrong subset value: \"{subset}\"')\n",
        "            raise ValueError(f'Wrong type of subset - {subset}. '\n",
        "                             f'Available types: {self.__reverse_dataset_type.keys()}')\n",
        "\n",
        "        if self.validation_split and 0.0 < self.validation_split < 1.0:\n",
        "            split = int(files_count * (1 - self.validation_split))\n",
        "            if subset == self.TRAINING:\n",
        "                files = files[:split]\n",
        "                json_files = json_files[:split]\n",
        "            else:\n",
        "                files = files[split:]\n",
        "                json_files = json_files[split:]\n",
        "\n",
        "        return SimpleFrameGenerator(rescale=self.rescale,\n",
        "                                    files=files,\n",
        "                                    shuffle=shuffle,\n",
        "                                    json_files=json_files,\n",
        "                                    *args, **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eazCkLeoNyFK"
      },
      "outputs": [],
      "source": [
        "data_gen = SimpleFrameDataGen(\n",
        "    validation_split=VALIDATION_SPLIT, \n",
        "    frame_glob_path=IMAGE_PATH+'/*/*.jpg', \n",
        "    json_hdf5_glob_path=JSON_HDF5_DATASET_PATH+'/*/*.hdf5',\n",
        ")\n",
        "\n",
        "train_generator = data_gen.flow_from_directory(\n",
        "  subset='training', shuffle=True, batch_size = BATCH_SIZE, \n",
        "  number_files=AMOUNT_OF_FRAMES, max_lines_per_frame=MAX_LINES_PER_FRAME,\n",
        "  max_num_points = MAX_NUM_POINTS, num_type_of_lines = NUM_TYPE_OF_LINES\n",
        ")\n",
        "\n",
        "validation_generator = data_gen.flow_from_directory(\n",
        "  subset='validation', shuffle=True, batch_size = BATCH_SIZE, \n",
        "  number_files=AMOUNT_OF_FRAMES, max_lines_per_frame=MAX_LINES_PER_FRAME,\n",
        "  max_num_points = MAX_NUM_POINTS, num_type_of_lines = NUM_TYPE_OF_LINES\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqKywRZMrMCr"
      },
      "outputs": [],
      "source": [
        "# for image_polylines in validation_generator:\n",
        "#     print(image_polylines[0].shape)\n",
        "#     print(image_polylines[1][0].shape)\n",
        "#     print(image_polylines[1][1].shape)\n",
        "#     break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zVvmiEN9R_M"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "model_name = 'lane_line_cnn_model'\n",
        "\n",
        "def build_model(polyline_output_shape:int ,label_output_shape:int, input_shape=(1280, 960, 3)):\n",
        "\n",
        "    # pretrained\n",
        "    pre_trained_model = tf.keras.applications.InceptionResNetV2(input_shape=input_shape,\n",
        "                            weights='imagenet',\n",
        "                            include_top=False)  \n",
        "    global_max_pool = layers.GlobalMaxPool2D()(pre_trained_model.output)\n",
        "    dropout_max_pool = layers.Dropout(.2)(global_max_pool)\n",
        "    \n",
        "    # polyline part\n",
        "    dense_polyline = tf.keras.layers.Dense(units=512, activation='relu')(dropout_max_pool)\n",
        "    batch_norm = layers.BatchNormalization()(dense_polyline)\n",
        "    dropout_polyine = layers.Dropout(.2)(batch_norm)\n",
        "\n",
        "    dense_polyline_2 = tf.keras.layers.Dense(units=512, activation='relu')(dropout_polyine)\n",
        "    batch_norm2 = layers.BatchNormalization()(dense_polyline_2)\n",
        "    dropout_polyine_2 = layers.Dropout(.2)(batch_norm2)\n",
        "\n",
        "    # label common part\n",
        "    dense_label = tf.keras.layers.Dense(units=256, activation='relu')(dropout_max_pool)\n",
        "    batch_norm = layers.BatchNormalization()(dense_label)\n",
        "    dropout_label = layers.Dropout(.2)(batch_norm)\n",
        "\n",
        "    # lane 1 part\n",
        "    x = tf.keras.layers.Dense(units=128, activation='relu')(dropout_label)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(.2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(units=64, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(.2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(units=32, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(.2)(x)\n",
        "   \n",
        "    # lane 2 part\n",
        "    y = tf.keras.layers.Dense(units=128, activation='relu')(dropout_label)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "    y = layers.Dropout(.2)(y)\n",
        "\n",
        "    \n",
        "    y = tf.keras.layers.Dense(units=64, activation='relu')(y)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "    y = layers.Dropout(.2)(y)\n",
        "\n",
        "    \n",
        "    y = tf.keras.layers.Dense(units=32, activation='relu')(y)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "    y = layers.Dropout(.2)(y)\n",
        "\n",
        "    # output\n",
        "    polyline_output = layers.Dense(polyline_output_shape,name='polyline_output')(dropout_polyine_2)\n",
        "    label_output_1 = layers.Dense(label_output_shape, activation='softmax', name='label_output_1')(x)\n",
        "    label_output_2 = layers.Dense(label_output_shape, activation='softmax', name='label_output_2')(y)\n",
        "\n",
        "    model = Model(pre_trained_model.input, outputs=[\n",
        "        polyline_output,\n",
        "        label_output_1,\n",
        "        label_output_2,\n",
        "      ], name = model_name\n",
        "    )\n",
        "\n",
        "    return model, pre_trained_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E86MF8po1u_p"
      },
      "outputs": [],
      "source": [
        "logdir = f\"logs/{model_name}\"\n",
        "checkpoint_filepath = f\"/model/{model_name}\"\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "early_stop_polyline_callback = tf.keras.callbacks.EarlyStopping(patience=8, monitor='val_polyline_output_loss')\n",
        "\n",
        "reduce_lr_callback_depends_on_polyline_loss = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_polyline_output_loss', factor=0.8, patience=3, verbose=1, mode='auto',\n",
        "    min_delta=0.0001, cooldown=0, min_lr=0.00001\n",
        ")\n",
        "reduce_lr_callback_depends_on_label_1_loss = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_label_output_1_loss', factor=0.8, patience=3, verbose=1, mode='auto',\n",
        "    min_delta=0.0001, cooldown=0, min_lr=0.00001\n",
        ")\n",
        "reduce_lr_callback_depends_on_label_2_loss = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_label_output_2_loss', factor=0.8, patience=3, verbose=1, mode='auto',\n",
        "    min_delta=0.0001, cooldown=0, min_lr=0.00001\n",
        ")\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_polyline_output_loss',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCnz_gehb-JL"
      },
      "outputs": [],
      "source": [
        "model, pre_trained_model = build_model(\n",
        "    polyline_output_shape=MAX_NUM_POINTS * 2 * MAX_LINES_PER_FRAME, \n",
        "    label_output_shape=NUM_TYPE_OF_LINES, \n",
        "    input_shape = INPUT_SHAPE)\n",
        "# print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t-RucGn0E4P"
      },
      "outputs": [],
      "source": [
        "pre_trained_model.trainable = False\n",
        "# print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjwwwR25pM5X"
      },
      "outputs": [],
      "source": [
        "# tf.experimental.numpy.experimental_enable_numpy_behavior()\n",
        "# tf.keras.utils.plot_model(model, \"multi_output_model.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ASFtrskei1b"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "model.compile(loss= {\n",
        "      'polyline_output':tf.keras.losses.MeanSquaredLogarithmicError(),\n",
        "      'label_output_1':tf.keras.losses.CategoricalCrossentropy(),\n",
        "      'label_output_2':tf.keras.losses.CategoricalCrossentropy(),\n",
        "    },\n",
        "    optimizer=Adam(learning_rate=learning_rate),\n",
        "    metrics={'polyline_output':tf.keras.metrics.MeanSquaredLogarithmicError(),\n",
        "             'label_output_1':'accuracy',\n",
        "             'label_output_2':'accuracy',\n",
        "             },)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRNqOCizesay"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_generator,\n",
        "                    epochs=30,\n",
        "                    verbose=2,\n",
        "                    validation_data=validation_generator,\n",
        "                    callbacks=[\n",
        "                        tensorboard_callback,\n",
        "                        early_stop_polyline_callback,\n",
        "                        reduce_lr_callback_depends_on_polyline_loss,\n",
        "                        model_checkpoint_callback,\n",
        "                        reduce_lr_callback_depends_on_label_1_loss,\n",
        "                        reduce_lr_callback_depends_on_label_2_loss,\n",
        "                      ],\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXNNF1iL2nJB"
      },
      "outputs": [],
      "source": [
        "# loss, polyline_output_loss,label_output_loss, polyline_output_mean_squared_logarithmic_error, label_output_accuracy = model.evaluate(validation_generator)\n",
        "# print(f\" \\\n",
        "#   Loss:{loss}\\n \\\n",
        "#   polyline_output_loss:{polyline_output_loss}\\n \\\n",
        "#   label_output_loss:{label_output_loss}\\n \\\n",
        "#   polyline_output_mean_squared_logarithmic_error:{polyline_output_mean_squared_logarithmic_error}\\n \\\n",
        "#   label_output_accuracy:{label_output_accuracy}\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKYNrqGcwiZv"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idAb55Mi2IWJ"
      },
      "outputs": [],
      "source": [
        "# !mkdir -p saved_model\n",
        "# model.save('saved_model/my_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4JC9Mr24hF5"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWVYqvsp6ZMC"
      },
      "outputs": [],
      "source": [
        "# files.download(\"saved_model/my_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WX33CM7f0g4O"
      },
      "outputs": [],
      "source": [
        "model_weight_name = f'{model_name}-weights.h5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15WhaJLJ6pSc"
      },
      "outputs": [],
      "source": [
        "# model.save(model_name) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxGvffvO7ELT"
      },
      "outputs": [],
      "source": [
        "# files.download(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2Jgjaci-sET"
      },
      "outputs": [],
      "source": [
        "model.save_weights(model_weight_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6rjRhAf-zVp"
      },
      "outputs": [],
      "source": [
        "files.download(model_weight_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP_WCXLf-3oc"
      },
      "outputs": [],
      "source": [
        "res = model.predict(validation_generator[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ75TNcXFkBX"
      },
      "outputs": [],
      "source": [
        "res[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcFCDmsmYTLY"
      },
      "outputs": [],
      "source": [
        "res[1][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6b9JZCYYpvB"
      },
      "outputs": [],
      "source": [
        "res[2][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiIIWkkOFoxC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "vl_100_hdf5_json_lane_recognition_3_output.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}